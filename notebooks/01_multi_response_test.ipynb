{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70387ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Response 1 ---\n",
      "\n",
      "Overfitting is a common issue that occurs when a machine learning model learns the training data too well, to the point where it performs poorly on unseen data (such as validation or test sets). In other words, the model becomes excessively complex and starts fitting not just the underlying pattern in the training data, but also the noise or outliers.\n",
      "\n",
      "Overfitting can occur when a model is too complex relative to the amount of training data available, or when it's trained for too long. This results in a model that fits the training data extremely well, but struggles to generalize to new, unseen data.\n",
      "\n",
      "Some signs of overfitting include:\n",
      "1. High accuracy on the training set but poor performance on the validation set or test set.\n",
      "2. The model performs well on easy examples from the training set, but poorly on harder examples that the model has not seen before.\n",
      "3. A high variance in the model's predictions, meaning that the model's predictions fluctuate a lot depending on small changes to the data.\n",
      "4. The model may learn to recognize patterns in the noise of the training data that are irrelevant to the actual problem being solved.\n",
      "\n",
      "To address overfitting, several techniques can be employed:\n",
      "1. Regularization: This involves adding a penalty term to the loss function, encouraging simpler models with fewer parameters.\n",
      "2. Early stopping: This involves monitoring the validation error during training and stopping when it starts to increase, before the model has a chance to overfit.\n",
      "3. Increasing the amount of training data.\n",
      "4. Using simpler models or ensemble methods that combine multiple simpler models to reduce complexity and improve generalization.\n",
      "5. Data augmentation: This involves artificially increasing the size of the training set by applying transformations like rotation, scaling, and flipping to existing images.\n",
      "6. Cross-validation: This is a technique for evaluating the performance of a model using multiple subsets of the data for training, validation, and testing purposes.\n",
      "\n",
      "--- Response 2 ---\n",
      "\n",
      "Overfitting is a common problem that occurs during the training of machine learning models, particularly when they are exposed to too much data or complex models are used for simple problems.\n",
      "\n",
      "In simpler terms, overfitting happens when a model learns the details and noise in the training data to such an extent that it negatively impacts its ability to make accurate predictions on new, unseen data (also known as generalization).\n",
      "\n",
      "The model becomes too specialized or complex, capturing not just the underlying patterns but also the random fluctuations and outliers present in the training data. As a result, the model struggles to make accurate predictions outside of the specific context it was trained on, often leading to poor performance on validation or test sets.\n",
      "\n",
      "To prevent overfitting, various techniques can be employed such as:\n",
      "\n",
      "1. Regularization (L1 and L2 regularization)\n",
      "2. Early stopping\n",
      "3. Cross-validation\n",
      "4. Dropout and data augmentation\n",
      "5. Reducing the complexity of the model by using simpler architectures or fewer layers and parameters.\n",
      "\n",
      "--- Response 3 ---\n",
      "\n",
      "Overfitting is a common issue in machine learning where a model learns the training data too well, to the point that it performs poorly on unseen data (like validation or test sets). This happens when the model becomes excessively complex and starts to fit not just the underlying patterns of the training data but also the noise or outliers present in the data.\n",
      "\n",
      "In simpler terms, overfitting occurs when a machine learning algorithm learns the training data so well that it begins to remember random fluctuations in the data instead of generalizing the pattern. As a result, the model performs very well on the training dataset but fails to make accurate predictions on new, unseen data.\n",
      "\n",
      "Overfitting can be caused by several factors:\n",
      "\n",
      "1. Having too many features or parameters compared to the amount of training data.\n",
      "2. Using complex models (e.g., high-degree polynomial regression or deep neural networks) for simple problems.\n",
      "3. Lack of regularization, which helps to prevent overfitting by adding a penalty term to the loss function to discourage large weights or complex patterns in the model.\n",
      "4. Insufficient shuffling and normalization of training data.\n",
      "5. Using only a small amount of training data.\n",
      "\n",
      "To avoid overfitting, various techniques can be employed:\n",
      "\n",
      "1. Cross-validation: Splitting the dataset into multiple folds and validating the model on each fold to get an average performance estimate.\n",
      "2. Regularization: Adding a penalty term to the loss function to prevent large weights or complex patterns in the model. Examples include L1 and L2 regularization for linear models, dropout for neural networks.\n",
      "3. Early stopping: Stopping the training process before the model starts to overfit by monitoring the performance on a validation set during training.\n",
      "4. Ensemble methods: Combining multiple simple models to improve generalization and reduce overfitting, such as bagging, boosting, and stacking.\n",
      "5. Data augmentation: Creating new synthetic data from existing data to increase the size of the dataset and help the model learn more robust features.\n",
      "\n",
      "--- Response 4 ---\n",
      "\n",
      "Overfitting is a common problem that occurs when a machine learning model learns the training data too well, to the point where it performs poorly on new, unseen data. In other words, the model becomes too complex and starts fitting not just the underlying patterns in the training data, but also the noise and outliers present in that data.\n",
      "\n",
      "When a model is overfitting, it may perform exceptionally well on the training dataset, achieving high accuracy and low error rates. However, when tested on new data (validation or test set), its performance degrades significantly due to its inability to generalize beyond the specific patterns of the training data.\n",
      "\n",
      "Overfitting can happen for various reasons such as having too many features relative to the number of samples, using overly complex models, insufficient regularization, or lack of proper validation techniques during model development. To address overfitting, techniques like cross-validation, early stopping, dropout, L1 and L2 regularization, and ensemble methods are commonly used in machine learning.\n",
      "\n",
      "--- Response 5 ---\n",
      "\n",
      "Overfitting is a common problem that occurs when a machine learning model is trained too well on its training data, to the point where it performs poorly on unseen data (such as validation or test data). This happens because the model becomes too complex and starts to fit not only the underlying pattern of the training data but also the noise and outliers present in the data.\n",
      "\n",
      "In simpler terms, overfitting occurs when a machine learning algorithm learns the specific details of the training data instead of learning general patterns that can be applied to new, unseen data. This makes the model less accurate and more prone to errors when used for prediction or classification on new examples.\n",
      "\n",
      "To prevent overfitting, various techniques can be employed, such as regularization (adding a penalty term to the loss function), early stopping (stopping the training process before the model starts to overfit), data augmentation (artificially increasing the size of the training set by modifying existing examples) and using simpler models like Ridge Regression or Logistic Regression instead of more complex ones.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.llm.response_generator import generate_multiple_responses\n",
    "\n",
    "prompt = \"Explain what overfitting is in machine learning.\"\n",
    "\n",
    "responses = generate_multiple_responses(prompt, n=5)\n",
    "\n",
    "for i, r in enumerate(responses):\n",
    "    print(f\"\\n--- Response {i+1} ---\\n\")\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eb45747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797985d2bd394adb94331f053137e189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consistency Score: 0.89655083\n"
     ]
    }
   ],
   "source": [
    "from src.embeddings.embedding_engine import EmbeddingEngine\n",
    "\n",
    "engine = EmbeddingEngine()\n",
    "\n",
    "embeddings = engine.encode(responses)\n",
    "similarity_matrix = engine.compute_similarity_matrix(embeddings)\n",
    "\n",
    "consistency_score = engine.compute_consistency_score(similarity_matrix)\n",
    "\n",
    "print(\"Consistency Score:\", consistency_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c60d8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Answer:\n",
      " Overfitting is a common issue that occurs when a machine learning model learns the training data too well, to the point where it performs poorly on unseen data (such as validation or test sets). In other words, the model becomes excessively complex and starts fitting not just the underlying pattern in the training data, but also the noise or outliers.\n",
      "\n",
      "Overfitting can occur when a model is too complex relative to the amount of training data available, or when it's trained for too long. This results in a model that fits the training data extremely well, but struggles to generalize to new, unseen data.\n",
      "\n",
      "Some signs of overfitting include:\n",
      "1. High accuracy on the training set but poor performance on the validation set or test set.\n",
      "2. The model performs well on easy examples from the training set, but poorly on harder examples that the model has not seen before.\n",
      "3. A high variance in the model's predictions, meaning that the model's predictions fluctuate a lot depending on small changes to the data.\n",
      "4. The model may learn to recognize patterns in the noise of the training data that are irrelevant to the actual problem being solved.\n",
      "\n",
      "To address overfitting, several techniques can be employed:\n",
      "1. Regularization: This involves adding a penalty term to the loss function, encouraging simpler models with fewer parameters.\n",
      "2. Early stopping: This involves monitoring the validation error during training and stopping when it starts to increase, before the model has a chance to overfit.\n",
      "3. Increasing the amount of training data.\n",
      "4. Using simpler models or ensemble methods that combine multiple simpler models to reduce complexity and improve generalization.\n",
      "5. Data augmentation: This involves artificially increasing the size of the training set by applying transformations like rotation, scaling, and flipping to existing images.\n",
      "6. Cross-validation: This is a technique for evaluating the performance of a model using multiple subsets of the data for training, validation, and testing purposes.\n",
      "\n",
      "Self-Critique:\n",
      " - Possible assumptions:\n",
      "  1. The reader has a basic understanding of machine learning concepts such as overfitting, loss function, regularization, early stopping, ensemble methods, and cross-validation.\n",
      "  2. The data used for training, validation, and testing is representative of the real-world problem being solved.\n",
      "  3. The model's performance on unseen data is the primary concern, as overfitting can lead to poor generalization.\n",
      "\n",
      "- Missing information:\n",
      "  1. No specific context or problem statement is provided, making it unclear which machine learning algorithm or dataset the answer pertains to.\n",
      "  2. The answer does not discuss the potential consequences of overfitting in real-world applications, such as financial losses or safety risks.\n",
      "\n",
      "- Potential weaknesses:\n",
      "  1. The answer focuses on techniques to prevent overfitting but does not provide a clear explanation of why overfitting occurs or how it can be detected beyond the signs listed.\n",
      "  2. The answer assumes that increasing the amount of training data will always help address overfitting, but this may not always be the case, especially when dealing with high-dimensional data or limited resources.\n",
      "\n",
      "- Areas of uncertainty:\n",
      "  1. The effectiveness of each technique for addressing overfitting can vary depending on the specific machine learning problem and dataset being used.\n",
      "  2. The answer does not discuss potential trade-offs between model complexity and performance, such as when to prioritize a more complex model with better accuracy on training data versus a simpler model that generalizes better to unseen data.\n",
      "\n",
      "Overall, while the answer provides a good overview of techniques for addressing overfitting in machine learning models, it could benefit from a clearer explanation of why overfitting occurs and how it can be detected, as well as discussing potential trade-offs between model complexity and performance.\n"
     ]
    }
   ],
   "source": [
    "from src.trust.self_critique import generate_self_critique\n",
    "\n",
    "critique = generate_self_critique(responses[0])\n",
    "\n",
    "print(\"Original Answer:\\n\", responses[0])\n",
    "print(\"\\nSelf-Critique:\\n\", critique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3df95f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ff3d30ab764874945f5626f2955a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Self-Critique Score: 0.2064296156167984\n"
     ]
    }
   ],
   "source": [
    "from src.trust.self_critique import SemanticVulnerabilityScorer\n",
    "\n",
    "scorer = SemanticVulnerabilityScorer()\n",
    "\n",
    "semantic_self_score = scorer.compute_score(critique)\n",
    "\n",
    "print(\"Semantic Self-Critique Score:\", semantic_self_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b121bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Trust Score: 0.8583459\n"
     ]
    }
   ],
   "source": [
    "from src.trust.trust_signals import TrustSignals\n",
    "\n",
    "signals = TrustSignals(\n",
    "    consistency_score=consistency_score,\n",
    "    vulnerability_score=semantic_self_score\n",
    ")\n",
    "\n",
    "new_trust = signals.compute_final_score()\n",
    "\n",
    "print(\"Updated Trust Score:\", new_trust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1af846a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fe795a46a8417a92df8fe0418373f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718f6422ac1c47c6b22763bc30dcc4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9711b41cdc8b4e51bf150d6f1f15d6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trust across variants: [np.float32(0.8211827), np.float32(0.84009165), np.float32(0.8125754)]\n"
     ]
    }
   ],
   "source": [
    "from src.llm.prompt_variants import generate_prompt_variants\n",
    "\n",
    "variants = generate_prompt_variants(prompt)\n",
    "\n",
    "variant_trust_scores = []\n",
    "\n",
    "for v in variants:\n",
    "\n",
    "    responses = generate_multiple_responses(v, n=5)\n",
    "\n",
    "    embeddings = engine.encode(responses)\n",
    "    sim_matrix = engine.compute_similarity_matrix(embeddings)\n",
    "    consistency = engine.compute_consistency_score(sim_matrix)\n",
    "\n",
    "    critique = generate_self_critique(responses[0])\n",
    "\n",
    "    scorer = SemanticVulnerabilityScorer()\n",
    "    vulnerability = scorer.compute_score(critique)\n",
    "\n",
    "    signals = TrustSignals(\n",
    "        consistency_score=consistency,\n",
    "        vulnerability_score=vulnerability\n",
    "    )\n",
    "\n",
    "    trust = signals.compute_final_score()\n",
    "\n",
    "    variant_trust_scores.append(trust)\n",
    "\n",
    "print(\"Trust across variants:\", variant_trust_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d2bdc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Trust: 0.8246166\n",
      "Trust Std Dev: 0.011492882\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mean_trust = np.mean(variant_trust_scores)\n",
    "std_trust = np.std(variant_trust_scores)\n",
    "\n",
    "print(\"Mean Trust:\", mean_trust)\n",
    "print(\"Trust Std Dev:\", std_trust)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_trust",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
